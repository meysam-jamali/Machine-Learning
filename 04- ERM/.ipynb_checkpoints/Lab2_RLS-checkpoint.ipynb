{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Regularized Least Square\n",
    "In this lab, we focus on RLS to address linear regression problems. \n",
    "\n",
    "In this lab, we have to:\n",
    "- **(Task 1)** implement RLS to solve linear regression problems\n",
    "- **(Task 2)** observe performance of RLS changing the noise in the data and the regularization parameter\n",
    "- **(Task 3)** implement K-Fold Cross-Validation algorithm for RLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "To generate linear regression data, we use the `linearRegrFunction` introduced in Lab0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegrFunction(n, D, low_D, high_D, W, sigma_noise):\n",
    "    X = np.zeros((n,D))\n",
    "    for i in range(0, D):\n",
    "        X[:,i] = np.random.uniform(low_D[i], high_D[i], size=n)\n",
    "    \n",
    "    gauss_noise = np.random.normal(0, sigma_noise, size=(n,1))\n",
    "\n",
    "    Y = np.dot(X, W) + gauss_noise\n",
    "    \n",
    "    return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Noiseless dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "D = 1\n",
    "low_D, high_D = [-1], [1]\n",
    "w = np.array(1.0).reshape(1, 1)\n",
    "noise_std = 0.0\n",
    "\n",
    "# Data generation\n",
    "X, Y = linearRegrFunction(n, D, low_D, high_D, w, noise_std)\n",
    "\n",
    "# Plot of the data\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Linear Regression data without noise\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: RLS regressor\n",
    "We want to implement the `regularizedLSTrain` function which train RLS regression.\n",
    "\n",
    "The signature of `regularizedLSTrain` is the following:\n",
    "\n",
    "`w = regularizedLSTrain(Xtr, Ytr, lam)`\n",
    "\n",
    "where:\n",
    "- **Xtr** are the training inputs\n",
    "- **Ytr** are the training outputs\n",
    "- **lam** is the regularization parameter $\\lambda$\n",
    "\n",
    "To implement this function, you will need to use the following functions from numpy:\n",
    "\n",
    "- [`np.linalg.cholesky`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html)\n",
    "- [`scipy.linalg.solve_triangular`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve_triangular.html)\n",
    "\n",
    "Consider \n",
    "\n",
    "**$(X_{tr}^\\intercal X_{tr} + \\lambda n I)w = X_{tr}^\\intercal Y_{tr}$**\n",
    "\n",
    "Let $A = X_{tr}^\\intercal X_{tr} + \\lambda n I$ and $b = X_{tr}^\\intercal Y_{tr}$, we can find $w$ with the following steps:\n",
    "1. First build the left-hand side matrix `A`, and the right-hand side matrix `b`.\n",
    "2. Compute the Cholesky decomposition of `A` (note that the numpy function will provide a lower-triangular matrix)\n",
    "3. You will have to solve two triangular systems, one using the Cholesky decomposition, and the other using its transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTrain(Xtr, Ytr, lam):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need also to implement a function `regularizedLSTest` which given a test set `Xte` and the `w` obtained using `regularizedLSTrain`, it returns `Ypred` containing the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularizedLSTest(w, Xte):\n",
    "    # Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of RLS regressor, we need a function to estimate the error.\n",
    "\n",
    "Given two vectors `Ytrue` (real outputs) and `Ypred` (predicted outputs), we can measure the error obtained when predicting `Ypred` instead of `Ytrue` with the MSE (Mean Square Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcError(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build an easy example to observe how our model works:\n",
    "- Generate a training set with **ntrain** points and a test set with **ntest** points \n",
    "- Train RLS with `regularizedLSTrain` function and test it with `regularizedLSTest` on test set\n",
    "- Compute the training and test error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 100\n",
    "ntest = 50\n",
    "D = 1\n",
    "low_D = [-1] * D\n",
    "high_D = [5] * D\n",
    "rnd_state = np.random.RandomState(42)\n",
    "wtrue = rnd_state.randn(D, 1) \n",
    "noise_std = 0.1\n",
    "\n",
    "lam = 1e-3\n",
    "\n",
    "# Generate a training set with ntrain points and a test set with ntest \n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "# Train RLS\n",
    "w = ...\n",
    "\n",
    "# Compute predictions on training and test set\n",
    "Ytr_pred = ...\n",
    "Yte_pred = ...\n",
    "\n",
    "train_err = ...\n",
    "test_err = ...\n",
    "\n",
    "print(\"[--] Training error: {}\\tTest error: {}\".format(train_err, test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Changing $\\lambda$\n",
    "Now we can play with our model changing the noise level in the data and changing the $\\lambda$ parameter.\n",
    "\n",
    "Let's start by changing $\\lambda$ and fixing the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 500\n",
    "ntest = 100\n",
    "D = 2\n",
    "low_D = [-2] * D\n",
    "high_D = [2] * D\n",
    "w_true = np.array([i for i in range(D)]).reshape(D, 1) + 10\n",
    "noise_std = 0.01\n",
    "\n",
    "\n",
    "# Data generation\n",
    "Xtr, Ytr = ...\n",
    "Xte, Yte = ...\n",
    "\n",
    "lam_list = np.logspace(-9, 1, 15)\n",
    "tr_err = []\n",
    "te_err = []\n",
    "\n",
    "for lam in lam_list:\n",
    "\n",
    "    # Train RLS\n",
    "    \n",
    "    # Compute predictions on training and test set\n",
    "    \n",
    "    # Compute training and test error and store them on tr_err and te_err\n",
    "\n",
    "\n",
    "# Plot training and test error\n",
    "_, ax = plt.subplots()\n",
    "ax.set_title(\"Training/Test error\")\n",
    "ax.plot(lam_list, tr_err, '-', c=\"blue\", label=\"training error\")\n",
    "ax.plot(lam_list, te_err, '-', c=\"orange\", label=\"test error\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain here what happens when $\\lambda$ increases: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, increase the amount of noise and repeat the experiment. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: K-Fold Cross Validation for RLS\n",
    "Now, we want to implement the K-Fold Cross Validation for RLS. \n",
    "\n",
    "In specific we want to implement the `KFoldCVRLS` function which, given a training set **Xtr** and **Ytr**, a number of folds **KF** and a set of values for $\\lambda$ (**regpar_list**) and returns the $\\lambda$ which minimize the average validation error **bestlam**, the mean validation error **val_mean**, the validation error variance **val_var**, the mean training error **tr_mean** and the training error variance **tr_var**.\n",
    "\n",
    "`bestlam, val_mean, val_var, tr_mean, tr_var = KFoldCVRLS(Xtr, Ytr, KF, regpar_list)`\n",
    "\n",
    "**Hint:** this function is very similar to K-Fold Cross-Validation algorithm for KNN (Lab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KFoldCVRLS(Xtr, Ytr, KF, regpar_list):\n",
    "    if KF <= 1:\n",
    "        raise Exception(\"Please supply a number of fold > 1\")\n",
    "\n",
    "    # Ensures that regpar_list is a numpy array\n",
    "    regpar_list = np.array(regpar_list)\n",
    "    num_regpar = regpar_list.size\n",
    "\n",
    "    n_tot = Xtr.shape[0]\n",
    "    n_val = int(n_tot // KF)\n",
    "\n",
    "    # We want to compute 1 error for each `k` and each fold\n",
    "    tr_errors = np.zeros((num_regpar, KF))\n",
    "    val_errors = np.zeros((num_regpar, KF))\n",
    "\n",
    "    for idx, regpar in enumerate(regpar_list):\n",
    "        # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "        rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "        split_idx = np.array_split(rand_idx, KF)\n",
    "        for fold in range(KF):\n",
    "            # Set the indices in boolean mask for all validation samples to `True`\n",
    "            val_mask = np.zeros(n_tot, dtype=bool)\n",
    "            val_mask[split_idx[fold]] = True\n",
    "\n",
    "            # Use the boolean mask to split X, Y in training and validation part\n",
    "\n",
    "            X = ... # training input \n",
    "            Y = ... # training output \n",
    "            X_val = ... # validation input\n",
    "            Y_val = ... # validation output\n",
    "            \n",
    "            # Train a RLS model for a single fold, and the given value of `regpar`\n",
    "            currW = ...\n",
    "            \n",
    "            # Compute the training error of the RLS regression for the given value of regpar\n",
    "            YpredTR = ...\n",
    "            tr_errors[idx, fold] = calcError(YpredTR, Y)\n",
    "\n",
    "            # Compute the validation error of the RLS regression for the given value of regpar\n",
    "            YpredVAL = ...\n",
    "            val_errors[idx, fold] = calcError(YpredVAL, Y_val)\n",
    "            \n",
    "    # Calculate error statistics along the repetitions\n",
    "    tr_mean = np.mean(tr_errors, axis=1)\n",
    "    tr_var = np.var(tr_errors, axis=1)\n",
    "    val_mean = np.mean(val_errors, axis=1)\n",
    "    val_var = np.var(val_errors, axis=1)\n",
    "    \n",
    "    bestlam_idx = np.argmin(val_mean)\n",
    "    bestlam = regpar_list[bestlam_idx]\n",
    "\n",
    "    return bestlam, val_mean, val_var, tr_mean, tr_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `KFoldCVRLS` to find the best regularization parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "D = 1\n",
    "sigma_noise = 0.5\n",
    "truew = np.random.randn(D, 1)\n",
    "reg_pars = np.logspace(-5, 1, 100)\n",
    "KF = 5\n",
    "\n",
    "low_D = [-3] * D\n",
    "high_D = [3] * D\n",
    "\n",
    "# Generate training set\n",
    "Xtr, Ytr =  ...\n",
    "\n",
    "# Compute best lambda\n",
    "bestlam, Vm, Vs, Tm, Ts = ...\n",
    "\n",
    "\n",
    "# Plot training and validation error\n",
    "_, ax = plt.subplots()\n",
    "ax.plot(reg_pars, Vm, '-o', label=\"Validation error\")\n",
    "ax.plot(reg_pars, Tm, '-o', label=\"Train error\")\n",
    "ax.axvline(bestlam, linestyle=\"--\", c=\"red\", alpha=0.7, label=\"best $\\lambda$\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"$\\lambda$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the best $\\lambda$ found to train the model on the full training set and compute the test error on the following test set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte, Yte = linearRegrFunction(200, D, low_D, high_D, truew, sigma_noise)\n",
    "\n",
    "print(\"[--] best lambda found: {}\".format(bestlam))\n",
    "\n",
    "# Insert your code here\n",
    "w_best = ...\n",
    "Ypred_best = regularizedLSTest(w_best, Xte)\n",
    "test_err = ...\n",
    "\n",
    "print(\"[--] Test error: {}\".format(test_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changes if you repeat the latter step with a value of Lambda which is not the optimal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To go deeper\n",
    "\n",
    "Create new training **and** test datasets, sampled in a non-symmetric range (for example you can set the `low_D` and `high_D` parameters of the `linearRegrFunction` function to 2 and 5).\n",
    "\n",
    "Then repeat the K-fold CV procedure, and check whether the best regularization parameter changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f26523b67ea2835ee6a77b36d2cc412a491957c6cdc7ecd6fb71972c20460352"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
