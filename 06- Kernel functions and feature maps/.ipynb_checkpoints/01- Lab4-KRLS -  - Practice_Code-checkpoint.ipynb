{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Kernel Regularized Least Squares\n",
    "\n",
    "In this lab activity we consider the extension of regularized least squares to non-linear problems using kernel functions.\n",
    "\n",
    "A brief summary of the tasks:\n",
    " 1. Generate a simple non-linear data-set\n",
    " 2. Use **linear** RLS to try and learn with such dataset\n",
    " 3. Use a **feature transformation** for learning with non-linear data\n",
    " 4. Implement various kernel functions\n",
    " 5. Implement kernel RLS\n",
    " 6. Generate a more complex non-linear data-set\n",
    " 7. Use kernel RLS for learning on non-linear data, use cross-validation to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n",
    "import scipy.spatial\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_err(Ypred, Ytrue):\n",
    "    return np.mean((Ypred-Ytrue)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Learning with a simple non-linear dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quadratic data\n",
    "\n",
    "In this lab we are going to use regression datasets where the target `Y` is **not a linear function** of the inputs `X`.\n",
    "\n",
    "As a first example, see the following function to generate quadratic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_data_gen(n, w, sigma_noise):\n",
    "    X = np.random.uniform(-3, 3, size=(n, 1))\n",
    "    Xsq = X ** 2\n",
    "    noise = np.random.normal(0, sigma_noise, size=(n, 1))\n",
    "\n",
    "    # Here we can use scalar multiplication since in dimension 1\n",
    "    Y = Xsq * w + noise\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "print(f\"Shape of x: {x_tr.shape}, shape of y: {y_tr.shape}\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Linear RLS\n",
    "\n",
    "The implementation of linear regularized least-squares is given below in the functions `rls_train` and `rls_predict`. \n",
    "\n",
    "Remember: regularized least-squares has the following form\n",
    "\n",
    "$$(X_{tr}^\\top X_{tr} + \\lambda n I)w = X_{tr}^\\top Y_{tr}$$\n",
    "\n",
    "\n",
    "Tasks:\n",
    " - Use RLS to train a linear regressor for the quadratic data. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rls_train(x, y, reg_par):\n",
    "    cov = x.T @ x + reg_par * x.shape[0] * np.eye(x.shape[1])\n",
    "    rhs = x.T @ y\n",
    "    w = np.linalg.solve(cov, rhs)\n",
    "    return w\n",
    "\n",
    "def rls_predict(x, w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "w = ...\n",
    "pred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Feature transform\n",
    "\n",
    "There is a simple way to use a linear algorithm for learning non-linear data: transforming the input data in such a way to convert the problem into a linear one.\n",
    "\n",
    "This is a simple fix in some cases, but becomes cumbersome if the datasets are non-linear in a complex way.\n",
    "\n",
    "Here we adopt this approach to train a RLS classifier with the quadratic dataset:\n",
    " 1. Generate the quadratic dataset\n",
    " 2. Transform the data so that it becomes a (n, 2) matrix containing the original input, and a transformed version of itself. Clearly the correct transformation depends on the underlying function (a quadratic function!).\n",
    " 3. Use the RLS algorithm on the new dataset\n",
    " 4. Plot **and comment** on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 0.5)\n",
    "\n",
    "# Instead of just the given xs build a feature matrix with the xs and their squares:\n",
    "trsf_x_tr = np.concatenate([...], axis=1)\n",
    "assert trsf_x_tr.shape == (x_tr.shape[0], 2), f\"Shape of x_tr is {x_tr.shape}. Expected ({x_tr.shape[0]}, 2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train a linear regression function on the new data!\n",
    "trsf_w_rls = ...\n",
    "pred_tr = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x_tr, y_tr, label=\"True\")\n",
    "ax.scatter(x_tr, pred_tr, label=\"Predicted\")\n",
    "ax.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Kernel Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement 3 types of kernel\n",
    "\n",
    "The `kernel_matrix` function takes as input two arrays of data, and outputs the kernel matrix evaluated at every pair of points.\n",
    "\n",
    "You should implement it using the formulas seen in class for the following kernels:\n",
    " - linear kernel -- here the `param` argument can be ignored\n",
    " - polynomial kernel -- here the `param` argument is the exponent of the kernel\n",
    " - gaussian kernel -- here the `param` argument is the kernel length-scale ($\\sigma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_distances(X1, X2):\n",
    "    \"\"\"Compute the matrix of pairwise squared-distances between all points in X1 and in X2.\n",
    "    \"\"\"\n",
    "    return scipy.spatial.distance.cdist(X1, X2, metric='seuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(X1, X2, kernel_type, param):\n",
    "    # X1 : array of shape n x d\n",
    "    # X2 : array of shape m x d\n",
    "    if kernel_type == 'linear':\n",
    "        return ...\n",
    "    elif kernel_type == 'polynomial':\n",
    "        exponent = param\n",
    "        return ...\n",
    "    elif kernel_type == 'gaussian':\n",
    "        lengthscale = param\n",
    "        return ...\n",
    "    else:\n",
    "        raise ValueError(kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Visualize the kernel (e.g. of the Gaussian type) for random data with different length-scales. What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.randn(100, 5)\n",
    "lengthscale = ...\n",
    "K = kernel_matrix(D, D, \"gaussian\", lengthscale)\n",
    "fig, ax = plt.subplots()\n",
    "ax.matshow(K)\n",
    "ax.set_title(\"Gaussian kernel with sigma=%f\" % (lengthscale))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Kernel RLS\n",
    "\n",
    "Remember that, given kernel $K = k(x_i, x_j)$ for $i=(1, \\dots, n)$ and $j=(1, \\dots, n)$, KRLS learns a weight-vector with the following formula\n",
    "\n",
    "$$(K + n \\lambda I)w = Y$$\n",
    "\n",
    "and then predictions on some new point $\\tilde{x}$ are given by\n",
    "\n",
    "$$f^{\\mathrm{KRLS}}(\\tilde{x}) = k(\\tilde{x}, X^{\\mathrm{train}}) w$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def krls_train(x, y, reg_par, kernel_type, kernel_par):\n",
    "    w = np.linalg.solve(...)\n",
    "    return w\n",
    "def krls_predict(x_ts, x_tr, w, kernel_type, kernel_par):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the functions you have implemented on the quadratic dataset.\n",
    "\n",
    "**Tasks:**\n",
    " 1. use the linear kernel, can you fit the data?\n",
    " 2. use the polynomial kernel, test the effect of the kernel parameter on the results:\n",
    "    - describe what happens with a low/high exponent in terms of the bias-variance tradeoff.\n",
    " 3. use the polynomial kernel, but fix the kernel parameter. Test the effect of the regularization parameter\n",
    "    - describe what happens with a low/high regularizer in terms of the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, y_tr = quadratic_data_gen(100, np.array([3]), 2.0)\n",
    "\n",
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a more complex non-linear dataset\n",
    "\n",
    "Define a function to generate a d-dimensional synthetic dataset where the targets `Y` depend non-linearly on the variables `X`.\n",
    "\n",
    "The parameters of the function are:\n",
    " - n : the number of samples\n",
    " - d : the dimension of the samples\n",
    " - low_d : the lower-bound for the uniformly distributed samples\n",
    " - high_d : the higher-bound for the uniformly distributed samples\n",
    " - sigma_noise : standard deviation of Gaussian noise added to the targets\n",
    " \n",
    "It should return:\n",
    " - X : 2D array of size n, d which fits in the desired bounds\n",
    " - Y : 2D array of size n, 1 which is a non-linear function of `X` (and a linear function of `w`)\n",
    " \n",
    "Examples of non-linear regression functions:\n",
    " - polynomial dependence of the Y on the X data\n",
    " - logarithmic dependence\n",
    " - more complex transforms (e.g. trig functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlinear_data_gen(n, d, low_d, high_d, sigma_noise):\n",
    "    X = np.random.uniform(low=low_d, high=high_d, size=(n, d))\n",
    "    assert X.shape == (n, d), \"Shape of X incorrect\"\n",
    "    \n",
    "    Y = ...\n",
    "    assert Y.shape == (n, 1), \"Shape of Y incorrect\"\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, sigma_noise, size=(n, 1))\n",
    "    Y_noisy = Y + noise\n",
    "    \n",
    "    return X, Y_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KRLS and cross-validation\n",
    "Now we will use KRLS and k-fold CV to learn on the complex non-linear datasets we have generated.\n",
    "\n",
    "In this last part you will have to do the following:\n",
    " 1. Generate a non-linear dataset, find the best hyperparameters (using e.g. the Gaussian kernel) with the training set, and then use them to make predictions on the test-set. Are you able to fit your data well?\n",
    " 2. Analyse how the amount of noise (see the `sigma_noise` parameter of `nonlinear_data_gen`) influences the best lambda as selected by cross-validation. In particular answer to the following question:\n",
    "     - How does the best lambda change as you increase/decrease the amount of noise in your dataset? Why? \n",
    "     \n",
    "    **Hint: keep the kernel parameter fixed for this third task, otherwise it might be very hard to see**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def krls_kfold_valerr(x_tr, y_tr, num_folds, reg_par, kernel_type, kernel_par):\n",
    "    \"\"\"\n",
    "    Compute the k-fold cross-validation error for one KRLS model (with speficied regularization, \n",
    "    kernel and kernel parameter).\n",
    "    \n",
    "    This function returns both the training errors and the validation errors \n",
    "    obtained from CV (as numpy arrays).\n",
    "    \"\"\"\n",
    "    if num_folds <= 1:\n",
    "        raise Exception(\"Please supply a number of folds > 1\")\n",
    "\n",
    "    n_tot = x_tr.shape[0]\n",
    "    n_val = int(n_tot // num_folds)\n",
    "    \n",
    "    tr_errs, val_errs = [], []\n",
    "    # `split_idx`: a list of arrays, each containing the validation indices for 1 fold\n",
    "    rand_idx = np.random.choice(n_tot, size=n_tot, replace=False)\n",
    "    split_idx = np.array_split(rand_idx, num_folds)\n",
    "    for fold in range(num_folds):\n",
    "        # Set the indices in boolean mask for all validation samples to `True`\n",
    "        val_mask = np.zeros(n_tot, dtype=bool)\n",
    "        val_mask[split_idx[fold]] = True\n",
    "        \n",
    "        kf_x_tr = x_tr[~val_mask]\n",
    "        kf_y_tr = y_tr[~val_mask]\n",
    "        kf_x_val = x_tr[val_mask]\n",
    "        kf_y_val = y_tr[val_mask]\n",
    "        \n",
    "        w_krls = krls_train(kf_x_tr, kf_y_tr, reg_par=reg_par, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        \n",
    "        pred_tr = krls_predict(kf_x_tr, kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        pred_val = krls_predict(kf_x_val, kf_x_tr, w_krls, kernel_type=kernel_type, kernel_par=kernel_par)\n",
    "        tr_errs.append(calc_err(pred_tr, kf_y_tr))\n",
    "        val_errs.append(calc_err(pred_val, kf_y_val))\n",
    "    return np.asarray(tr_errs), np.asarray(val_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def krls_kfoldcv(x_tr, y_tr, num_folds, reg_par_list, kernel_type, kernel_par_list):\n",
    "    \"\"\"Choose the best parameters for both the regularizer and the kernel parameter according to K-Fold CV.\n",
    "    \"\"\"\n",
    "    errors = np.zeros((len(reg_par_list), len(kernel_par_list)))\n",
    "    for i, reg_par in enumerate(reg_par_list):\n",
    "        for j, kernel_par in enumerate(kernel_par_list):\n",
    "            tr_error, val_error = krls_kfold_valerr(x_tr, y_tr, num_folds, reg_par, kernel_type, kernel_par)\n",
    "            errors[i][j] = np.mean(val_error)\n",
    "            \n",
    "    best_reg_par = reg_par_list[np.unravel_index(np.argmin(errors), errors.shape)[0]]\n",
    "    best_kernel_par = kernel_par_list[np.unravel_index(np.argmin(errors), errors.shape)[1]]\n",
    "    best_err = np.min(errors)\n",
    "    print(f\"The best error (MSE={best_err*100:.2f}%) was obtained with \"\n",
    "          f\"lambda={best_reg_par}, kernel-parameter={best_kernel_par}\")\n",
    "    return best_reg_par, best_kernel_par, best_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyse the effect of noise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
